---
sidebar_position: 1
---

# Module 4: Vision-Language-Action (VLA)

Welcome to Module 4 of the Physical AI & Humanoid Robotics course. This module covers Whisper for voice-to-command, LLM-based cognitive planning, and natural language to ROS 2 actions conversion.

## Overview

The Vision-Language-Action (VLA) module focuses on creating natural interfaces for humanoid robots using voice commands, large language models for cognitive planning, and converting natural language to executable ROS 2 actions.

## What You'll Learn

In this module, you will learn about:

- **Whisper Integration**: Converting human speech into text commands for robot processing
- **LLM-Based Cognitive Planning**: Using Large Language Models for high-level reasoning and decision making
- **Natural Language to Actions**: Converting natural language commands to executable ROS 2 actions

## Chapters

- [Chapter 1: Whisper for Voice Commands](./chapter-1-whisper-voice-commands.md)
- [Chapter 2: LLM-Based Cognitive Planning](./chapter-2-llm-cognitive-planning.md)
- [Chapter 3: Natural Language to Actions](./chapter-3-natural-language-actions.md)

The VLA system represents the culmination of the Physical AI & Humanoid Robotics course, combining all previous modules into a complete system that can understand and respond to human commands naturally.